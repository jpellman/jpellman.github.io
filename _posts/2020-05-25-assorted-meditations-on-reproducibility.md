---
layout: post
title:  "Assorted Reflections on Reproducibility for 2020"
date:   2020-05-25 21:36:14
permalink: /2020-reproducibility.html
---

It's been a while since I've written about scientific reproducibility, and I've accumulated quite a few perspectives in the intervening months.  I'm not certain that my thoughts have achieved a sufficient coherence to merit a full essay, but I believe that even in an unrefined form, they are worth publishing on this site as part of the broader dialogue about scientific reproducibility.  Most of the short motifs that follow are simply responses to other articles that are out there that I've come across.  It's important to note beforehand that I am mostly a layman in the topics that I comment on, and that I have not been fully active in participating in the context that they are being debated within.  Anything I say should thus be taken with a grain of salt.

# The National Academies of Science and Engineering and ACM Definitions of Reproducibility

In November of last year, I attended the [SC19](https://sc19.supercomputing.org/) conference, which brings together an assortment of computer scientists, systems administrators, and vendors.  One of the various birds of a feather sessions I attended ([The National Academiesâ€™ Report on Reproducibility and Replicability in Science: Inspirations for the SC Reproducibility Initiative](https://sc19.supercomputing.org/session/?sess=sess293)) discussed the issue of reproducibility in great detail.  The big news at this session was that several [operationalizations](https://en.wikipedia.org/wiki/Operationalization) for the concept of reproducibility that were developed independently by the ACM (see [here](https://www.acm.org/publications/policies/artifact-review-badging)) and the National Academies (see [here](https://doi.org/c5jp)) were being harmonized across the two organizations.

A primary motivation for this reconciliation of jargon was that the ACM and National Academies were, in fact, using the same words to refer to the nearly the exact opposite concepts.  Namely, the ACM defined the term **replicability** to mean an attempt to reproduce an article's results using the exact same experimental setup by another team, while the term **reproducilibity** indicated the concept of convergent results made by a different team using a different experimental setup.  

The National Academies, in contrast, defined **reproducibility** as the act of an independent team obtaining the same results through rigid adherence to the same methods and setup while also using the same dataset, while **replicability** referred to convergent results emerging from non-identical setups.

My personal take on this news is that the original operationalizations used by both the ACM and the National Academies are both flawed, not by dint of their actual definitions, but in the manner that they have been formulated.  Both sets of definitions treat varying degrees of reproducibility as belonging to discrete categories, whereas I think that a better characterization of reproducibility would be to think of it as a continuous spectrum.  Very rarely are replication attempts as black or white as to be binned into one or two categories, and by trying to lump studies together in this way valuable details about how a replication was performed could be lost.

It is my personal opinion that it would be better to use one term (reproducibility) consistently, and maybe treat other terms such "replicability" as redundant aliases for the main term, and then to characterize reproducibility as being composed of several continuous features (such as similarity between measurement apparatuses or computing environments) that can be reported alongside a replication attempt.

# On How Computers Did (or Didn't) Break Science

Recently I came across [an article](https://theconversation.com/how-computers-broke-science-and-what-we-can-do-to-fix-it-49938) that impugns electronic computers for breaking science.  Namely, computers are blamed for:

 * Making data processing methods more opaque and converting processes that were formerly transparent into black boxes.
 * Being too versatile, and thus complicating methods reporting in journal articles.  Furthermore, making it so that results are reproducible now involves documenting your both your software and dataset and potentially using a standardized organizational scheme like [BIDS](https://bids.neuroimaging.io/) or the [CF](http://cfconventions.org/Data/cf-documents/overview/article.pdf) metadata standard.

While the article contains many positions that I agree with to varying degrees, such as increased sharing of data, the use of open source toolkits, and a shift away from exclusively point-and-click applications, I find that the central premise- that computers in and of themselves have somehow disrupted the scientific process- to be poorly supported.  I do think that there is an element of truth to what the author says in this regard, but I think that this central position lacks precision.

From my perspective as a technical professional with a different mindset and different background, this article could sway me to one of two positions:

 * That scientists are simply misusing technology, and that the author's points merely indicate a mass demographic of [bad workmen/women blaming their tools](https://en.wiktionary.org/wiki/a_bad_workman_always_blames_his_tools).
 * That there is a fundamental issue with how computing interfaces are designed for scientists, and that this leads them to perform actions that are maladaptive.  In this position, computational reproducibility is fundamentally hampered by poor user interface design decisions rather than by the researchers themselves.  Scientific computation would benefit from more of what [Don Norman](https://en.wikipedia.org/wiki/Don_Norman) has called [user-centered design](https://en.wikipedia.org/wiki/User-centered_design).

The latter perspective, which I happen to agree with, doesn't directly implicate computers themselves, but more subtly implicates a particular aspect of computing that is problematic.  In this perspective, a whole computer (which is more than just its interface) is not responsible for science's woes, but rather an individual component is flawed.  For the particular concerns expressed by the author, we don't necessarily need to worry about adders and half-adders, since his concerns can be allayed with a more narrow scope.

The author also cites [a claim by Victoria Stodden](https://www.edge.org/annual-question/2014/response/25340) that a computer is fundamentally different from other pieces of scientific instrumentation in his article.  I am slightly skeptical of this claim, in no small part because many pieces of instrumentation actually contain full-blown onboard computers (similar to Raspberry Pis) that perform a portion of data processing to produce the "raw" data.  

Phrased differently, unless you're using analog instrumentation, your microscope almost certainly is running a full-fledged copy of Linux, a BSD, Windows, or [Minix](https://www.minix3.org/) to ensure that the output you receive is encoded in a digital format.  In fMRI research, MRI scanners don't even use onboard PCs, instead using whole workstations that perform some rudimentary image processing steps as part of data acquisition (i.e., [k-space](https://en.wikipedia.org/wiki/K-space_(magnetic_resonance_imaging)) transformations) that often go unreported and are typically forgotten about.

Even outside the realm of science, printers, ATMs, and [New York City subway kiosks](https://i.imgur.com/8qsaf.jpg) have been running complete copies of Windows for years.  Even components within a computer, such as hard disk controllers or CPUs, have become themselves computers running their own operating systems.  In 2017, [it was even revealed](https://www.zdnet.com/article/minix-intels-hidden-in-chip-operating-system/) that a large number of modern Intel processors have been secretly running the [Minix operating system](https://www.minix3.org/).

If the author is to critique computers for being too opaque, he cannot claim that most modern instrumentation is somehow fundamentally less opaque, since most modern instruments are actually application-specific digital computers.  In fact, such application-specific computers are arguably even more opaque than your average generalized computer, since the processes that they use to transform data into a "raw" format during data acquisition are often proprietary, undocumented by the instrument manufacturer, or both.

While I have not undertaken an exhaustive review to prove the contrary, I also suspect that the author's characterization of pre-electronic computer methods reporting as being more transparent to be an example of [rosy retrospection](https://en.wikipedia.org/wiki/Rosy_retrospection).  For instance, [Hodgkin and Huxley (1952)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/) is often considered an essential study in the history of neuroscience for its modeling of the action potential.  The methods section of this paper, while transparent in giving the necessary formulae to reproduce its results (these formulae would be analogous to analysis source code or a Jupyter notebook today), does not indicate how these formulae were applied.  Nowhere is it indicated if Hodgkin and Huxley (or a room of [human computers](https://en.wikipedia.org/wiki/Computer_(job_description))) calculated their results by hand or if they (more likely) used a mechanical calculator.  It is even feasible that Cambridge University's [EDSAC](https://en.wikipedia.org/wiki/EDSAC) vacuum tube computer was used for the preparation of Hodgkin and Huxley's seminal manuscript.  There's simply no obvious way of knowing based off the article alone, because Hodgkin and Huxley didn't think it necessary to have that level of transparency in their article.  In this respect, by modern standards of computational reproducibility, the Hodgkin and Huxley study is almost certainly more opaque.  Of course, we now know (from sources apart from the manuscript) that Hodgkin and Huxley had wanted to use EDSAC, but were forced to use a [Brunsviga](https://en.wikipedia.org/wiki/Odhner_Arithmometer) instead (see [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3424716/)) due to an extended maintenance window on EDSAC.
