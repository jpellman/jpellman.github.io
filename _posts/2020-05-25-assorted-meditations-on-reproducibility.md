---
layout: post
title:  "Assorted Reflections on Reproducibility for 2020"
date:   2020-05-25 21:36:14
permalink: /2020-reproducibility.html
---

It's been a while since I've written about scientific reproducibility, and I've accumulated quite a few perspectives in the intervening months.  I'm not certain that my thoughts have achieved a sufficient coherence to merit a full essay, but I believe that even in an unrefined form, they are worth publishing on this site as part of the broader dialogue about scientific reproducibility.  Most of the short motifs that follow are simply responses to other articles that are out there that I've come across.  It's important to note beforehand that, while I feel confident in commenting on technical topics, I admittedly have not been fully keeping up with all of the discourse about reproducibility, so there are doubtless some blind spots that may be affecting my perspective.

# The National Academies of Science and Engineering and ACM Definitions of Reproducibility

In November of last year, I attended the [SC19](https://sc19.supercomputing.org/) conference, which brings together an assortment of computer scientists, systems administrators, and vendors.  One of the various birds of a feather sessions I attended ([The National Academiesâ€™ Report on Reproducibility and Replicability in Science: Inspirations for the SC Reproducibility Initiative](https://sc19.supercomputing.org/session/?sess=sess293)) discussed the issue of reproducibility in great detail.  The big news at this session was that several [operationalizations](https://en.wikipedia.org/wiki/Operationalization) for the concept of reproducibility that were developed independently by the ACM (see [here](https://www.acm.org/publications/policies/artifact-review-badging)) and the National Academies (see [here](https://doi.org/c5jp)) were being harmonized across the two organizations.

A primary motivation for this reconciliation of jargon was that the ACM and National Academies were, in fact, using the same words to refer to the nearly the exact opposite concepts.  Namely, the ACM defined the term **replicability** to mean an attempt to reproduce an article's results using the exact same experimental setup by another team, while the term **reproducibility** indicated the concept of convergent results made by a different team using a different experimental setup.  

The National Academies, in contrast, defined **reproducibility** as the act of an independent team obtaining the same results through rigid adherence to the same methods and setup while also using the same dataset, while **replicability** referred to convergent results emerging from non-identical setups.

My personal take on this news is that the original operationalizations used by both the ACM and the National Academies are both flawed, not by dint of their actual definitions, but in the manner that they have been formulated.  Both sets of definitions treat varying degrees of reproducibility as belonging to discrete categories, whereas I think that a better characterization of reproducibility would be to think of it as a continuous spectrum.  Very rarely are replication attempts as black or white as to be binned into one or two categories, and by trying to lump studies together in this way valuable details about how a replication was performed could be lost.

It is my personal opinion that it would be better to use one term (reproducibility) consistently, and maybe treat other terms such "replicability" as redundant aliases for the main term, and then to characterize reproducibility as being composed of several continuous features (such as similarity between measurement apparatuses or computing environments) that can be reported alongside a replication attempt.

# On How Computers Did (or Didn't) Break Science

Recently I came across [an article](https://theconversation.com/how-computers-broke-science-and-what-we-can-do-to-fix-it-49938) that impugns electronic computers for breaking science.  Namely, computers are blamed for:

 * Making data processing methods more opaque and converting processes that were formerly transparent into black boxes.
 * Being too versatile, and thus complicating methods reporting in journal articles.  Furthermore, making it so that results are reproducible now involves documenting your both your software and data management efforts.

While the article contains many positions that I agree with to varying degrees, such as increased sharing of data, the use of open source toolkits, and a shift away from exclusively point-and-click applications, I find that the central premise- that computers in and of themselves have somehow disrupted the scientific process- to be poorly supported.  I do think that there is an element of truth to what the author says in this regard, but I think that this central position lacks precision.

From my perspective as a technical professional with a different mindset and different background from many researchers, I regard problems with computational reproducibility as originating from two separate causes:

 * **[Nondeterministic algorithms](https://en.wikipedia.org/wiki/Nondeterministic_algorithm)**, which do not necessarily produce the same outputs given the same input across multiple runs.
 * Human factors issues.

The article in question does not address mathematical causes of irreproducibility, and instead focuses on the human factors front.  From this vantage point, there are two possibilities for why computational reproducibility might be challenging:

 * That scientists are simply misusing technology, and that the author's points merely indicate a mass demographic of [bad workmen/women blaming their tools](https://en.wiktionary.org/wiki/a_bad_workman_always_blames_his_tools).
 * That there is a fundamental issue with how computing interfaces are designed for scientists, and that this leads them to perform actions that are maladaptive.  In this position, computational reproducibility is fundamentally hampered by poor user interface design decisions rather than by the researchers themselves.  Scientific computation would benefit from more of what [Don Norman](https://en.wikipedia.org/wiki/Don_Norman) has called [user-centered design](https://en.wikipedia.org/wiki/User-centered_design).

It is my belief that both possibilities are responsible for issues of computational reproducibility in varying proportions.  The remedy for the former possibility is more education, and it is for this reason (to transform bad workmen/women into good workmen/women) that efforts such as [Software Carpentry](https://software-carpentry.org/) exist.  In an ideal world, education about scientific computing would begin even earlier at the undergraduate level, since computing is becoming essential to all areas of research, and would be better learned before the competing burden of needing to publish scholarly articles comes into play.

The latter possibility is addressed by efforts such as [Jupyter Notebooks](https://jupyter.org/), [Galaxy](https://usegalaxy.org/), [brainlife](https://brainlife.io/), and [NeuroCaaS](http://www.neurocaas.org/), which simplify computing by abstracting away elements of general-purpose computing that are irrelevant to science while keeping elements that fit within a researcher's cognitive schema / understanding of the world.  Jupyter, for instance, uses a notebook analogy, similar to how a researcher might make notes in a literal notebook while performing benchwork.  The other tools perform specific tasks, in well-defined pipelines, with fixed inputs and outputs deliberately constraining the space of software elements that a researcher must manage while increasing the consistency of research outputs.  When running the [mriqc](https://mriqc.readthedocs.io/en/stable/) pipeline on brainlife, for instance, functionality is restricted to a clear and obvious goal- ensuring that data quality is acceptable.  While to some extent these are black boxes, they are also based upon incredibly transparent components that can be audited if need be- it is for this reason that I must clarify that I am not wholesale against the use of point and click applications, as long as such applications are built upon versatile and reasonably transparent components.

Human factors issues related to data management in particular are also being partially addressed by metadata standards such as [BIDS](https://bids.neuroimaging.io/), [NIDM](http://nidm.nidash.org/), [EML](https://eml.ecoinformatics.org/), and [CF Convetions](http://cfconventions.org/).  These standards encourage reproducibility by decreasing the number of possibilities that files on a filesystem can be organized, constraining researchers with a default set of good data management practices.  Efforts such as [Datajoint](https://datajoint.io/) go even further, encouraging researchers to manage data within structured database tables.  In the long-term, I believe that the data science world will come to influence data management practices within science positively, and that most analyses will be performed on data stored within highly structured databases in a transactional manner instead of on files directly, while files enriched with metadata schemas will come to be used as intermediate, portable representations of datasets that can be imported into databases via various connectors.

The author also cites [a claim by Victoria Stodden](https://www.edge.org/annual-question/2014/response/25340) that a computer is fundamentally different from other pieces of scientific instrumentation in his article.  I am slightly skeptical of this claim, in no small part because many pieces of instrumentation themselves contain full-blown onboard computers (similar to Raspberry Pis) that perform a portion of data processing to produce the "raw" data.  

Phrased differently, unless you're using analog instrumentation, your microscope almost certainly is running a full-fledged copy of Linux, a BSD, Windows, or [Minix](https://www.minix3.org/) to ensure that the output you receive is encoded in a digital format.  In fMRI research, MRI scanners don't even use onboard PCs, instead using whole workstations that perform some rudimentary image processing steps as part of data acquisition (i.e., [k-space](https://en.wikipedia.org/wiki/K-space_(magnetic_resonance_imaging)) transformations) that often go unreported and are typically forgotten about.

Even outside the realm of science, printers, ATMs, and [New York City subway kiosks](https://i.imgur.com/8qsaf.jpg) have been running complete copies of Windows for years.  Even components within a computer, such as hard disk controllers or CPUs, have become themselves computers running their own operating systems.  In 2017, [it was even revealed](https://www.zdnet.com/article/minix-intels-hidden-in-chip-operating-system/) that a large number of modern Intel processors have been secretly running the [Minix operating system](https://www.minix3.org/).

If the author is to critique computers for being too opaque, he cannot claim that most modern instrumentation is somehow fundamentally less opaque, since most modern instruments are actually application-specific digital computers.  In fact, such application-specific computers are arguably even more opaque than your average generalized computer, since the processes that they use to transform data into a "raw" format during data acquisition are often proprietary, undocumented by the instrument manufacturer, or both.

While I have not undertaken an exhaustive review to prove the contrary, I also suspect that the author's characterization of pre-electronic computer methods reporting as being more transparent to be an example of [rosy retrospection](https://en.wikipedia.org/wiki/Rosy_retrospection).  For instance, [Hodgkin and Huxley (1952)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/) is often considered an essential study in the history of neuroscience for its modeling of the action potential.  The methods section of this paper, while transparent in giving the necessary formulae to reproduce its results (these formulae would be analogous to analysis source code or a Jupyter notebook today), does not indicate how these formulae were applied.  Nowhere is it indicated if Hodgkin and Huxley (or a room of [human computers](https://en.wikipedia.org/wiki/Computer_(job_description))) calculated their results by hand or if they (more likely) used a mechanical calculator.  It is even feasible that Cambridge University's [EDSAC](https://en.wikipedia.org/wiki/EDSAC) vacuum tube computer was used for the preparation of Hodgkin and Huxley's seminal manuscript.  There's simply no obvious way of knowing based off the article alone, because Hodgkin and Huxley didn't think it necessary to have that level of transparency in their article.  In this respect, by modern standards of computational reproducibility, the Hodgkin and Huxley study is almost certainly more opaque.  Of course, we now know (from sources apart from the manuscript) that Hodgkin and Huxley had wanted to use EDSAC, but were forced to use a [Brunsviga](https://en.wikipedia.org/wiki/Odhner_Arithmometer) instead (see [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3424716/)) due to an extended maintenance window on EDSAC.
