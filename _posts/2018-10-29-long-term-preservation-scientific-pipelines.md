---
layout: post
title:  "Meditations on the 'Archivability Crisis' in Science and the Long-Term Reproducibility of Scientific Analyses"
date:   2018-10-29 23:59:59
permalink: /scientific-archivability.html
---

This post is a brief, non-comprehensive response to C. Titus Brown's [How I learned to stop worrying and love the coming archivability crisis in scientific software](http://ivory.idyll.org/blog/2017-pof-software-archivability.html), informed both by [Emulation & Virtualization as Preservation Strategies](https://mellon.org/media/filer_public/0c/3e/0c3eee7d-4166-4ba6-a767-6b42e6a1c2a7/rosenthal-emulation-2015.pdf) by David S. H. Rosenthal and past experiences attending [Vintage Computer Festival East](http://vcfed.org/wp/festivals/vintage-computer-festival-east/) in the lovely oceanside locale of Wall Township, NJ.  

As always, the following disclaimer applies (as it does to all of my opinions): I could be full of [whoomp](https://www.youtube.com/watch?v=1Gmn-XFGZws).  

I intend to react to several of Brown's assertions.  Namely: 1) that you can't save all the software necessary to faithfully reproduce an analysis pipeline, 2) that containers, as black boxes, are bad for inspectability, and 3) that analyses have a "half-life of utility", and this in turn renders near-literal reproducibility undesirable due to cost and effort.
 
I'll start by putting my own biases out for inspection- I'm candidly a lot more bullish about the long-term viability of replicating scientific experiments *in silica* going forward, and my gut reaction to this Brown's post is that the claim that we can't save *all* software, while superficially true in the sense that it would be near impossible to save the entire statistical population of all written software, is hyperbolic in the sense that the most popular scientific software stacks will be sufficiently preserved into the distant future.  Why do I think this?  Statistically, the more popular a scientific package is the more copies of it will be made, and the more likely it is to survive.  Essentially, I have confidence in a sort of digital Darwinism that will ensure that core packages essential to scientific analayses remain around for a while.

Beyond an evolutionary argument, modern software stacks have an advantage in the sense that most environments, whether they be Linux, BSD, or OS X (heck even [Windows](https://chocolatey.org/)) now have package managers that allow you to install specific versions of a piece of software.  So long as one mirrors the package repositories (or even just individual package files for dependencies) for all the applications and libraries used in an analysis, one can easily unpack pre-requisite software and re-create an environment for one's operating system of choice.  Prior to the development of [apt](https://en.wikipedia.org/wiki/APT_(Debian)) and [yum](https://en.wikipedia.org/wiki/Yum_(software)) in the late 90s, it is certainly true that software stacks were much harder to reproduce due to the homogeneity of software and hardware architectures and the inconsistency in how software was distributed.  Unfortunately, due to lack of awareness amongst scientists concerning the long-term advantages of proper packaging and the admittedly non-sexy work involved in maintaining package repositories this key tool has yet to be fully embraced, although there are notable efforts such as [the conda package manager](https://conda.io/docs/), [the Fedora SciTech SIG](https://fedoraproject.org/wiki/SIGs/SciTech/PackageList), [DebianScience](https://wiki.debian.org/DebianScience), [NeuroFedora](https://fedoraproject.org/wiki/SIGs/NeuroFedora) and [NeuroDebian](http://neuro.debian.net/).

There's also the fact that non-scientists, such as librarians and law enforcement officers, also have a vested interest in maintaining software stacks on a decades-long time scale.  Librarians do so to preserve cultural heritage.  Law enforcement, in contrast, does so for more practical and less abstract reasons.  Forensics specialists, in order to rapidly investigate born-digital evidence, have a need to have rapid access to both signatures identifying applications and important files.  To this end, NIST maintains a substantial collection of software packages as part of its [National Software Reference Library](https://www.nist.gov/software-quality-group/nsrl-introduction).  The public metadata for this collection indicates that a large number of popular pre-1999 scientific packages, such as MATLAB and SPSS, are already included in the collection.  Indeed, there are likely other lessons that academic scientists can learn from forensic scientists, whose analyses should ideally be reproducible in case of an appeal (if not superceded by newer techniques, as has been the case with genetic testing exonerating many innocent prisoners).

Thus far I've focused primarily on software itself, but what about lower levels of the stack, such as the operating system and hardware?  For legacy ecosystems such as SPARC, 68k, PPC or VAX, these lower layers can be faithfully reproduced through software emulation at a minimum, and through hardware-based strategies if greater performance is necessary.  New developments in emulation, while primarily driven by retrocomputing and gaming enthusiasts instead of scientific researchers, hold great promise for the long-term near-literal reproducibility of older analyses.  In the libraries digital preservation community, a distinction is made between two forms of fidelity when a digital artefact (such as a digital art exhibit programmed in HyperCard) is emulated: execution fidelity and experiential fidelity (Rosenthal, 2.4.3).  The former is trivially true due to [Turing equivalence](https://en.wikipedia.org/wiki/Turing_completeness).  As Rosenthal notes:

> In a Turing sense all computers are equivalent to each
> other, so it is possible for an emulator to replicate the behavior
> of the target machineâ€™s CPU and memory exactly,
> and most emulators do that. 

The latter form of fidelity cannot be implemented via emulation alone, but matters little for in silica reproducibility since any environmental context is irrelevant to the veracity of the results.  Experiential fidelity firmly falls into that category of things so impertinent to the experiment that they belong to statistical error at best (e.g., the color of Rutherford's tie when he first performed his gold foil experiment).  It simply does not matter whether or not a study's result is displayed on a monochrome Mac Plus CRT display or a modern MacBook Pro Retina monitor.

For modern analyses, it has become evident that two instruction set architectures, Intel/AMD x86_64 and ARM, underpin the vast majority of all computing due to market consolidation in the CPU space (Rosenthal, 3.2.4).  Within scientific computing in particular, this market consolidation is even more acute, with only 6.2% of the Top 500 supercomputers using architectures other than AMD/Intel's x86_64 as of June 2018 (see [here](https://www.top500.org/statistics/list/)). This means that, relative to the heterogeneity of hardware architectures in the 90s, the total space of hardware we would need to emulate in the future is quite small for a modern scientific analysis.  Furthermore, it is abundantly clear that the majority of Top 500 HPC clusters are running Linux, which indicates that there are very few degrees of freedom when it comes to operating system choice in modern scientific environments.  

In his blog, Brown cites how some researchers have proposed co-opting the software development concept of "continuous integration" as a potential solution for the reproducibility crisis, re-running analyses constantly as data is recorded.  While this concept is intriguing, I'd suggest that scientists adapt another concept from the software development world, [code coverage](https://en.wikipedia.org/wiki/Code_coverage), but with the "coverage" element not relating to how many functions in their code have tests, but rather how many components of their software/hardware stack will be likely be emulatable in the future.  While it's not certain that someone will write emulators that implement every single feature of an instruction set architecture in the future, this kind of "emulation coverage" might be informed by factors such as a) how niche the hardware is, b) how large the community of end-users is, and c) how well-documented the hardware's interfaces and underlying implementation are.

Beyond software-based emulation of older hardware, there's also been a growing trend of using FPGAs to implement older hardware directly.  There are numerous advantages to this approach, from better utilization of resources such as electricity and compute cycles (e.g., many cycles are idle when you're emulating a legacy machine on a powerful machine and nothing else is running in the background), to the ability to run hardware in cloud instances such as Amazon's [F1 instance types](https://aws.amazon.com/ec2/instance-types/f1/), and finally the ability to more closely moderate the speed of execution (deliberately slowing it down if necessary).  A good example of this in the education space is Stephan Edwards's [Apple2fpga](http://www.cs.columbia.edu/~sedwards/apple2fpga/), which re-implements an Apple II+ using an FPGA board.  This example also illustrates how the Apple II might be considered a platform with good emulation coverage, since as Edwards notes on his site:

> The Apple II has been documented in great detail. Starting with the first Apple II "Redbook" Reference Manual, Apple itself has published the schematics for the Apple II series. When Woz spoke at Columbia, he mentioned this was intentional: he wanted to share as much technical information as possible to educate the users. 

It should be noted that the trend towards FPGAs is in a way an example of how "everything old is new again".  At VCF East 2018, an exhibit entitled "[Microcomputers With an Identity Crisis](http://www.retrotechnology.com/vcfe18/vcfe_may18.html)" by Douglas Crawford, Chris Fala, and Todd George demonstrated how in the past, it used to be more common to have multiple hardware architectures running on a single machine as a way to mitigate against the large number of incompatible hardware architectures.  While the examples given at the exhibit were actually ASICs on add-on cards (such as the [Apple IIe compatibility card](https://en.wikipedia.org/wiki/Apple_IIe_Card) for the Mac Color Classic or the 486 included alongside the conventional PowerPC CPU in the [PowerMAC 6100](https://en.wikipedia.org/wiki/Power_Macintosh_6100).  These add-on cards resemble the FPGA cloud instances, in the sense that FPGA instances can run multiple architectures.  Even if x86_64 did become obsolete, it could be re-implemented in an FPGA and then run in the cloud on a machine simulating many architectures at once.

Software:
https://www.cs.drexel.edu/~bls96/eniac/
https://hackaday.com/2018/05/24/vcf-east-the-desktop-eniac/#more-308966

Basically, yes Docker is black box and that could be a bad thing.  You really need configuration management (which promotes Yolanda Gil's inspectability) with hardcoded version numbers to document your server setup, good software packaging practices (which make software stacks less brittle and software archivability easier to manage), and well-documented workflows (my ideal would be to use an easy to use format like CWL).

The issue with Docker also is that containerization is not sufficiently standardized across POSIX platforms.  If there's a mass exodus from Linux to illumos at some point, for example, the containerization solution would become a solaris zone.

Another point in this whole conversation about reproducibility, however, is how far we want to go with reproducing results.  Up until now, we've focused on talking about reproducing the CPU instructions / program that performs an analysis, but what about the data?  There's another fidelity at play here, instrumentation fidelity.  Are we going to re-create the exact same MRI machine or electron microscope to validate results in the future?  Many designs are proprietary in life sciences.  What if there are systematic flaws in data collection that affect an entire generation of analyses that we don't know about.

Data repositories aren't as well developed or standardized, though dat and others are trying to change this.
