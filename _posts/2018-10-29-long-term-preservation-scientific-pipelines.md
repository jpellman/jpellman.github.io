---
layout: post
title:  "Meditations on the 'Archivability Crisis' in Science and the Long-Term Reproducibility of Scientific Analyses"
date:   2018-10-29 23:59:59
permalink: /scientific-archivability.html
---

This post is a response to C. Titus Brown's [How I learned to stop worrying and love the coming archivability crisis in scientific software](http://ivory.idyll.org/blog/2017-pof-software-archivability.html), informed both by [Emulation & Virtualization as Preservation Strategies](https://mellon.org/media/filer_public/0c/3e/0c3eee7d-4166-4ba6-a767-6b42e6a1c2a7/rosenthal-emulation-2015.pdf) by David S. H. Rosenthal and past experiences attending [Vintage Computer Festival East](http://vcfed.org/wp/festivals/vintage-computer-festival-east/) in the lovely oceanside locale of [Wall Township, NJ](https://www.youtube.com/watch?v=Mskx6c1DULQ).  

I intend to react to several of Brown's assertions.  Namely: 1) that you can't save all the software necessary to faithfully reproduce an analysis pipeline, 2) that containers and VM images, as black boxes, are bad for inspectability, and 3) that analyses have a "half-life of utility", and this in turn renders literal reproducibility undesirable due to cost and effort.  Note that Brown's own views on this may have converged with my own at this point to some degree- I have not been able to fully keep up with his writings, so I apologize if he has made similar points to my own elsewhere.  A lot of what I say may reflect my more general belief that, in many respects, scientific reproducibility is a problem better tackled by engineering methodologies.

Software Preservation, Digital Darwinism, and the Role of Packaging Systems in Promoting Reproducibility
========================================================================================================
 
I'll start by putting my own biases out for inspection- I'm candidly a lot more bullish about the long-term viability of replicating scientific experiments *in silica* going forward, and my gut reaction to Brown's post is that the claim that we can't save *all* software, while superficially true in the sense that it would be near impossible to save the entire statistical population of all written software, is hyperbolic in the sense that the most popular scientific software stacks will be sufficiently preserved into the distant future.  Why do I think this?  Statistically, the more popular a software package is the more copies of it will be made, and the more likely it is to survive.  Furthermore, a substantial number of the software layers that scientific computing relies upon are not unique to science, and in being general purpose even more copies will be made by non-science actors.  Essentially, I have confidence in a sort of digital darwinism that will ensure that core packages essential to scientific analayses remain around for a while.  Whether or not this darwinism applies to binaries and source code equally is debatable, but I have confidence that most software will survive in some form through this manner (e.g., [LINPACK](http://www.netlib.org/linpack/index.html) in its 1988 incarnation is still downloadable today).

Beyond an evolutionary argument, modern software stacks have an advantage in the sense that most environments, whether they be Linux, BSD, or OS X (heck even [Windows](https://chocolatey.org/)) now have package managers that allow you to install specific versions of a piece of software.  So long as one mirrors the package repositories (or even just individual package files for dependencies) for all the applications and libraries used in an analysis, one can easily unpack pre-requisite software and re-create an environment for one's operating system of choice.  In the case of general purpose Linux distribution repositories at least, there are [several](http://vault.centos.org) [mirrors](http://archive.kernel.org/)  going back to at least the mid-2000s.  Prior to the development of [apt](https://en.wikipedia.org/wiki/APT_(Debian)) and [yum](https://en.wikipedia.org/wiki/Yum_(software)) in the late 90s, it is certainly true that software stacks were much harder to reproduce due to the homogeneity of software and hardware architectures and the inconsistency in how software was distributed.  Unfortunately, due to lack of awareness amongst scientists concerning the long-term advantages of proper packaging, the admittedly non-sexy work involved in maintaining package repositories, and time constraints due to the present publish or perish culture it seems as thought this key tool will not be fully embraced in the near future, although there are notable efforts such as [the conda package manager](https://conda.io/docs/), [the Fedora SciTech SIG](https://fedoraproject.org/wiki/SIGs/SciTech/PackageList), [DebianScience](https://wiki.debian.org/DebianScience), [NeuroFedora](https://fedoraproject.org/wiki/SIGs/NeuroFedora) and [NeuroDebian](http://neuro.debian.net/).


There's also the fact that non-scientists, such as librarians and law enforcement officers, also have a vested interest in maintaining software stacks on a decades-long time scale.  Librarians do so to preserve cultural heritage.  Law enforcement, in contrast, does so for more practical and less abstract reasons.  Forensics specialists, in order to rapidly investigate born-digital evidence, have a need to have rapid access to both signatures identifying applications and key files included with those applications.  To this end, NIST maintains a substantial collection of software packages as part of its [National Software Reference Library](https://www.nist.gov/software-quality-group/nsrl-introduction).  The public metadata for this collection indicates that a large number of popular pre-1999 scientific packages, such as MATLAB and SPSS, are already included in the collection.  

Hardware and Software Emulation 
===============================

Thus far I've focused primarily on software itself, but what about lower levels of the stack, such as the operating system and hardware?  For legacy ecosystems such as SPARC, 68k, PPC or VAX, these lower layers can be faithfully reproduced through software emulation at a minimum, and through hardware-based strategies if some other property (more precise performance, lower power consumption, etc) is necessary.  New developments in emulation, while primarily driven by retrocomputing and gaming enthusiasts instead of scientific researchers, hold great promise for the long-term near-literal reproducibility of older analyses (an [especially notable project](https://www.cs.drexel.edu/~bls96/eniac/) by Brian Stuart can even run reference programs for the ENIAC ).  To better contextualize my thoughts on this, I'd like to note the distinction between two forms of fidelity when a digital artefact (such as an analysis or digital art exhibit ) is emulated: execution fidelity and experiential fidelity (Rosenthal, 2.4.3).  The former is trivially true due to [Turing equivalence](https://en.wikipedia.org/wiki/Turing_completeness).  As David Rosenthal notes:

> In a Turing sense all computers are equivalent to each other, so it is possible for an emulator to replicate the behavior
> of the target machineâ€™s CPU and memory exactly, and most emulators do that. 

The latter form of fidelity cannot be implemented via emulation alone, but matters little for in silica reproducibility since any context related to the appearance of the hardware is irrelevant to the veracity of the results.  Experiential fidelity firmly falls into that category of things so impertinent to the experiment that they belong to statistical error at best (e.g., the color of Rutherford's tie when he first performed his gold foil experiment, if he even wore a tie).  It simply does not matter whether or not a study's result is displayed on a monochrome Mac Plus CRT display or a modern MacBook Pro Retina monitor.

It could be argued that while Turing equivalence makes it theoretically possible that any analysis can be reproduced literally, it may not be pragmatic to achieve this ideal.  It is for this reason that I have been referring to "near-literal" reproducibility throughout this post rather than "literal" reproducibility.  I believe that for scientific analyses at least, if we cannot have execution fidelity that is 100% accurate and precise, we should at the very least think of our analyses in terms of [engineering tolerance](https://en.wikipedia.org/wiki/Engineering_tolerance), in the sense that we would expect a result to occur within a certain interval a large percentage of the time.

For modern analyses, it has become evident that two instruction set architectures, Intel/AMD x86_64 and ARM, underpin the vast majority of all computing due to market consolidation in the CPU space (Rosenthal, 3.2.4).  Within scientific computing in particular, this market consolidation is even more acute, with only 6.2% of the Top 500 supercomputers using architectures other than AMD/Intel's x86_64 as of June 2018 (see [here](https://www.top500.org/statistics/list/)). This means that, relative to the heterogeneity of hardware architectures in the 90s, the total set of hardware we would need to emulate in the future is quite small for a modern scientific analysis.  Furthermore, it is abundantly clear that the majority of Top 500 HPC clusters are running Linux, which indicates that there are very few degrees of freedom when it comes to operating system choice in modern scientific environments.  

In his blog, Brown cites how some researchers have proposed co-opting the software development concept of "continuous integration" as a potential solution for the reproducibility crisis, re-running analyses constantly as data is recorded.  While this concept is intriguing, I'd suggest that scientists adapt another concept from the software development world, [code coverage](https://en.wikipedia.org/wiki/Code_coverage), but with the "coverage" element not relating to how many functions in their code have tests, but rather how many components of their software/hardware stack will likely be emulatable in the future.  While there's no guarantee that a software developer will write an emulator that implements every single feature of a given instruction set architecture in the future, this kind of "emulation coverage" might be informed by factors such as a) how niche the hardware is, b) how large the community of end-users is, and c) how well-documented the hardware's interfaces and underlying implementations are.

Beyond software-based emulation of older hardware, there's also been a growing trend of using FPGAs to implement older hardware directly.  There are numerous advantages to this approach, from better utilization of resources such as electricity and compute cycles (i.e., many cycles are idle when you're emulating a legacy machine on a powerful machine and nothing else is running in the background), to the ability to run custom hardware in the cloud via Amazon's [F1 instance types](https://aws.amazon.com/ec2/instance-types/f1/), and finally the ability to more closely moderate the speed of execution (deliberately slowing it down if necessary).  A good example of this in the education space is Stephan Edwards's [Apple2fpga](http://www.cs.columbia.edu/~sedwards/apple2fpga/), which re-implements an Apple II+ using an FPGA board.  This example also illustrates how the Apple II might be considered a platform with good emulation coverage, since as Edwards notes on his site:

> The Apple II has been documented in great detail. Starting with the first Apple II "Redbook" Reference Manual, Apple itself has published the schematics for the Apple II series. When Woz spoke at Columbia, he mentioned this was intentional: he wanted to share as much technical information as possible to educate the users. 

It should be noted that historically speaking, it wasn't uncommon to run multiple software architectures on a single machine.  At VCF East 2018, an exhibit entitled "[Microcomputers With an Identity Crisis](http://www.retrotechnology.com/vcfe18/vcfe_may18.html)" by Douglas Crawford, Chris Fala, and Todd George demonstrated how ASICs (such as the [Apple IIe compatibility card](https://en.wikipedia.org/wiki/Apple_IIe_Card) for the Mac Color Classic) and add-on cards (such as a 486 CPU that operated in tandem with a PowerPC processor in the [PowerMac 6100](https://en.wikipedia.org/wiki/Power_Macintosh_6100)) were used to consolidate the large number of incompatible hardware architectures during the 80s and 90s.  Modern cloud infrastructure with FPGA instances can serve a similar purpose in allowing multiple hardware architectures to be run alongside each other. It's foreseeable that even if the x86_64 architecture did become obsolete, it could be re-implemented on an FPGA and run in the cloud.

On Docker/Virtualization, Inspectability and Configuration Management Tooling
=============================================================================

I find myself in agreement with Brown's assessment of Docker, in that treating scientific software and analysis pipelines as black boxes is dangerous for reproducibility and reduces transparency, although admittedly it could be argued that [Dockerfiles](https://docs.docker.com/engine/reference/builder/) and [Docker Compose](https://docs.docker.com/compose/) fufill some of the requirements of inspectability that Brown advocates.  The primary issue I have with Dockerfiles for purposes of inspectability is that they aren't sufficiently portable, and can't be used to deploy to bare metal, virtual machines, or other containerization technologies.  Anecdotally, it seems that many researchers are in a rush to embrace Docker without considering that it could quite easily be supplanted by [Kata Containers](https://katacontainers.io/), [jails](https://en.wikipedia.org/wiki/FreeBSD_jail) or [zones](https://illumos.org/man/5/zones) should the tech industry embrace these alternative technologies (which owing to some of Docker's historical failings wouldn't surprise me).  This particular nitpick of mine is related to the "emulation coverage" I discussed earlier, where it's important to consider the long-term viability of a technology before employing it.

Instead of Dockerfiles, I think that the inspectability criterion should be fulfilled by configuration management tools such as [Ansible](https://www.ansible.com/), [Puppet](https://puppet.com/), [Chef](https://www.chef.io/chef/), and [Cfengine](https://cfengine.com/).  Configuration management tools use a declarative syntax that allows you to specify exactly how your environment is set up (i.e., which packages are installed, which versions are used, which hard disk volumes are mounted, etc).  While this syntax varies between tools, it can be used to both apply a series of commands against a base installation of an operating system to bring an execution host to a desired state, and can also serve as documentation that future researchers can look at and remix.  Intriguingly, Cfengine  was created with the explicit goal of setting up research environments for physicists in the early 90s, which consequently means that a large number of computing environments from that time period might be very well-characterized.

Conda environment.yml files also fulfill inspectability in a similar way to configuration management tools.  In fact, this is what HHMI's [Binder](https://mybinder.readthedocs.io/en/latest/config_files.html#config-files) tool uses to construct Docker images.  At this point, it's important to note that the amount of effort needed to reproduce a pipeline doesn't always necessarily need to focus on OS-level dependencies.  If a pipeline is implemented solely in a higher-level language such as Python, a researcher may have considerably less work in terms of documenting his/her environment.  Indeed, Docker is basically to C and C libraries what a Python virtualenv is to Python and Python packages.  If a researcher has no need for a specific version of a C library then a Python virtualenv may be adequate.

Finally, I believe that the inspectability criterion must be fulfilled by well-documented workflows that indicate how tools in an analysis fit together.  My opinion is that this would be best accomplished by a common syntax such as [Common Workflow Language](https://www.commonwl.org/) (CWL).  I've discussed my beliefs about the potential benefits of CWL at length [elsewhere](https://libjpel.so/cwl-support.html), so I won't re-hash them here, but the short of it is that I strongly believe in a neutral, platform-agnostic way for describing how tools fit together (essentially high-level programming).

Thoughts on Literal Reproducibility, and the Utility of Research Products
=========================================================================

While I agree with Brown that literal reproducibility is impractical, I strongly believe that the ability to reproduce analyses with the highest precision possible has value beyond the short-term.  For one, I think that there is a moral imperative for analyses to be repeatable, since most analyses are not produced with private funding, but rather governmental funding, and as such should be available to the general public in a form that they can inspect and run with relative ease (should they obtain the hardware resources necessary to do so).  I believe that this is essential for public instruction, to narrow the gap between scientists in an "ivory tower" and everyday citizens, to demonstrate good use of public funds, and to increase accountability / reduce scientific fraud.  I also believe that it makes science less vulnerable to attack, especially in the case of climate science since it's harder for climate science deniers to have as much traction if the exact analyses that they want to discredit are accessible and readily runnable in some fashion.

Additionally, I think that it's dangerous to make assumptions about the utility of exact repeatability (or anything else in science or life for that matter) in the long-term.  The value that we assign to any given research product, such as a reproducible analysis, is constantly in flux, and there's no reasonable way to predict the worth of that analysis at any given point in the distant future.  Historical judgements anticipating the future value of research products have led to extremely suboptimal decisions, such as the alleged [destruction of data](https://www.nytimes.com/2016/08/07/magazine/the-brain-that-couldnt-remember.html). As a quick disclaimer, I realize that the particular example I cite here h is controversial and that the jury is still out on whether any data was actually destroyed and that most likely no malicious intent was involved; nevertheless, destruction of research materials definitely does occur on some scale throughout science and can lead to great distrust in the court of public opinion.  Furthermore, value judgements about analyses have led to systematic issues within science itself such as the file drawer problem wherein null results are undervalued and thus never communicated to other scientists (at best).

It's also worth pointing out that the problem-solving process in science itself benefits from having a legacy analysis available for revival if necessary in the distant future.  In cognitive science, it is theorized that two methods of problem-solving, difference reduction and means-ends analysis, are employed to get to a goal state, such as an analysis-backed conclusion.  In difference reduction, or [hill climbing](https://en.wikipedia.org/wiki/Hill_climbing), actions are continually performed that minimize the difference between the current state and a desired goal state.  This works fine for simple problems, but has the caveat that a problem solver can get stuck in a rut (or local maxima) and not step back to go down a path that would lead them to the ideal goal state (a global maximum).   In means-ends analysis, multiple sub-goals are created as responses to blocking states as the problem-solving process occurs. These sub-goals are then considered separately to build a path to an overall goal state.  Means-ends analysis is the ideal problem-solving strategy; however, it assumes a well-characterized goal, which is often not the case in more exploratory scientific analyses.  Due to the high uncertainty of scientific outcomes, I would assume that difference reduction reasoning strategies are more prevalent than means-ends analysis in the scientific process.  Within the context of the computational reproducibility conversation, what this means is that a scientific field could go down a kind of [garden path](https://en.wikipedia.org/wiki/Garden-path_sentence) for a while, get stuck in a local maxima, and then need to back up to unblock itself.  If there is a need to back up to the point before a scientific paradigm branched towards a particular direction, we would want to be able re-run analyses from that exact point of branching to determine our next directions.

Other Considerations: Instrument Fidelity and the Role of Data in Reproducible Analyses
=======================================================================================

Up until now, I've focused on reproducing the CPU instructions / program that performs an analysis, but what about the data?  There's another kind of fidelity at play in computational analyses that can get lost in the discussion when computation is emphasized heavily.  In addition to the execution fidelity discussed earlier, ideally the equipment and methods used for data collection would be repeatable in the future as well. This would constitute a sort of instrumentation fidelity, and while it's not as important if we considered data to be fixed for purposes of repeatable versions of a single *in silica* analysis, I don't believe that we should consider collected data to be "hard-coded".  If an analysis is to be truly generalizable, it should also be able to produce results within the same "tolerance" with an entirely novel set of data.  In fact, this might be an interesting revision to the peer review process, wherein data gathered by a totally different lab could be input into a pipeline to verify results.

However, how would we achieve inspectability with data gathering instruments, and how could we ensure that the instrumentation itself wasn't flawed in some systematic way?  While Brown asserts that "closed source software is useless crap because it satisfies neither repeatability nor inspectability", it's also true that most equipment designs for life sciences are proprietary, and in many cases it's impossible to double-check for fundamental design flaws that might systematically distort the data a pipeline acts upon.  Though there are some efforts to create [open hardware](http://www.appropedia.org/Building_research_equipment_with_free,_open-source_hardware) for labs, in many cases instrumentation is too niche to have an open source equivalent or benefits from economies of scale that individual labs don't have.

Additionally, adding considerations about instrumentation fidelity opens up a number of other questions

Are we going to re-create the exact same MRI machine or electron microscope to validate results in the future?  Many designs are proprietary in life sciences.  What if there are systematic flaws in data collection that affect an entire generation of analyses that we don't know about?

Additionally, Data repositories aren't as well developed or standardized, though dat and others are trying to change this.

Summary
=======

In brief, my thoughts are that:

* Scientific reproducibility is, in many ways, more of an engineering problem than a scientific one.
* It seems unlikely that scientific software stacks will be incredibly difficult to preserve in many (if not most) cases.  Many elements of scientific software stacks, such as OS libraries, are more likely to be preserved due to their general purpose use.  Non-scientists such as librarians and law enforcement have a vested interest in preserving scientific software on a decades-long time scale.  Software packaging, if adopted more widely, would make reproducibility even easier to achieve, although at a minimum the use of version control tools such as [git](https://git-scm.com/) could provide some of the same functionality as packaging (such as the ability to install specific software versions via commit number).
* While literal reproducibility is impractical (though theoretically possible) establishing tolerance limits (or perhaps a more appropriate statistical concept) for acceptable outputs from a scientific analysis is less so.
* Hardware and software emulation of lower levels of the scientific computing stack should for the most part be adequate.  Researchers, when creating computational analyses, should consider how difficult it will be to emulate their analysis in the future ("emulation coverage").
* I agree that the creation of VM images or Docker containers are a non-solution to the reproducbility of in silica scientific analyses.  In lieu of relying solely on images and containers, I think that it is more important to document environments via configuration management tools or other environment specifications.  These tools provide for both high inspectability and can be used to re-create environments on top of a base operating system installation.
* Systematic documentation of workflows via a syntax such as Common Workflow Language (CWL) is essential for providing inspectability and documentation for how software components are linked together in an analysis.
* Changes in instrumentation for data collection also throw a ratchet into the problem of reproducibility, especially since many tools used to collect data are proprietary and data repositories don't have the same well-established tooling that software packages have in terms of packaging or version control.
