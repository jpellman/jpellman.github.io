---
layout: post
title:  "Meditations on the 'Archivability Crisis' in Science and the Long-Term Reproducibility of Scientific Analyses"
date:   2018-10-29 23:59:59
permalink: /scientific-archivability.html
---

This post is a brief, non-comprehensive response to C. Titus Brown's [How I learned to stop worrying and love the coming archivability crisis in scientific software](http://ivory.idyll.org/blog/2017-pof-software-archivability.html), informed both by [Emulation & Virtualization as Preservation Strategies](https://mellon.org/media/filer_public/0c/3e/0c3eee7d-4166-4ba6-a767-6b42e6a1c2a7/rosenthal-emulation-2015.pdf) by David S. H. Rosenthal and past experiences attending [Vintage Computer Festival East](http://vcfed.org/wp/festivals/vintage-computer-festival-east/) in the lovely oceanside locale of [Wall Township, NJ](https://www.youtube.com/watch?v=Mskx6c1DULQ).  

As always, the following disclaimer applies (as it does to all of my opinions): I could be full of [whoomp](https://www.youtube.com/watch?v=1Gmn-XFGZws).  

I intend to react to several of Brown's assertions.  Namely: 1) that you can't save all the software necessary to faithfully reproduce an analysis pipeline, 2) that containers, as black boxes, are bad for inspectability, and 3) that analyses have a "half-life of utility", and this in turn renders near-literal reproducibility undesirable due to cost and effort.
 
I'll start by putting my own biases out for inspection- I'm candidly a lot more bullish about the long-term viability of replicating scientific experiments *in silica* going forward, and my gut reaction to Brown's post is that the claim that we can't save *all* software, while superficially true in the sense that it would be near impossible to save the entire statistical population of all written software, is hyperbolic in the sense that the most popular scientific software stacks will be sufficiently preserved into the distant future.  Why do I think this?  Statistically, the more popular a scientific package is the more copies of it will be made, and the more likely it is to survive.  Essentially, I have confidence in a sort of digital Darwinism that will ensure that core packages essential to scientific analayses remain around for a while.

Beyond an evolutionary argument, modern software stacks have an advantage in the sense that most environments, whether they be Linux, BSD, or OS X (heck even [Windows](https://chocolatey.org/)) now have package managers that allow you to install specific versions of a piece of software.  So long as one mirrors the package repositories (or even just individual package files for dependencies) for all the applications and libraries used in an analysis, one can easily unpack pre-requisite software and re-create an environment for one's operating system of choice.  Prior to the development of [apt](https://en.wikipedia.org/wiki/APT_(Debian)) and [yum](https://en.wikipedia.org/wiki/Yum_(software)) in the late 90s, it is certainly true that software stacks were much harder to reproduce due to the homogeneity of software and hardware architectures and the inconsistency in how software was distributed.  Unfortunately, due to lack of awareness amongst scientists concerning the long-term advantages of proper packaging and the admittedly non-sexy work involved in maintaining package repositories this key tool has yet to be fully embraced, although there are notable efforts such as [the conda package manager](https://conda.io/docs/), [the Fedora SciTech SIG](https://fedoraproject.org/wiki/SIGs/SciTech/PackageList), [DebianScience](https://wiki.debian.org/DebianScience), [NeuroFedora](https://fedoraproject.org/wiki/SIGs/NeuroFedora) and [NeuroDebian](http://neuro.debian.net/).

There's also the fact that non-scientists, such as librarians and law enforcement officers, also have a vested interest in maintaining software stacks on a decades-long time scale.  Librarians do so to preserve cultural heritage.  Law enforcement, in contrast, does so for more practical and less abstract reasons.  Forensics specialists, in order to rapidly investigate born-digital evidence, have a need to have rapid access to both signatures identifying applications and important files.  To this end, NIST maintains a substantial collection of software packages as part of its [National Software Reference Library](https://www.nist.gov/software-quality-group/nsrl-introduction).  The public metadata for this collection indicates that a large number of popular pre-1999 scientific packages, such as MATLAB and SPSS, are already included in the collection.  Indeed, there are likely other lessons that academic scientists can learn from forensic scientists, whose analyses should ideally be reproducible in case of an appeal (if not superceded by newer techniques, as has been the case with genetic testing exonerating many innocent prisoners).

Thus far I've focused primarily on software itself, but what about lower levels of the stack, such as the operating system and hardware?  For legacy ecosystems such as SPARC, 68k, PPC or VAX, these lower layers can be faithfully reproduced through software emulation at a minimum, and through hardware-based strategies if some other property (more precise performance, lower power consumption, etc) is necessary.  New developments in emulation, while primarily driven by retrocomputing and gaming enthusiasts instead of scientific researchers, hold great promise for the long-term near-literal reproducibility of older analyses (an [especially notable project](https://www.cs.drexel.edu/~bls96/eniac/) by Brian Stuart can even run reference programs for ENIAC ).  To better contextualize my thoughts on this, I'd like to note the distinction between two forms of fidelity when a digital artefact (such as a digital art exhibit programmed in HyperCard) is emulated: execution fidelity and experiential fidelity (Rosenthal, 2.4.3).  The former is trivially true due to [Turing equivalence](https://en.wikipedia.org/wiki/Turing_completeness).  As Rosenthal notes:


> In a Turing sense all computers are equivalent to each
> other, so it is possible for an emulator to replicate the behavior
> of the target machineâ€™s CPU and memory exactly,
> and most emulators do that. 

The latter form of fidelity cannot be implemented via emulation alone, but matters little for in silica reproducibility since any environmental context is irrelevant to the veracity of the results.  Experiential fidelity firmly falls into that category of things so impertinent to the experiment that they belong to statistical error at best (e.g., the color of Rutherford's tie when he first performed his gold foil experiment).  It simply does not matter whether or not a study's result is displayed on a monochrome Mac Plus CRT display or a modern MacBook Pro Retina monitor.  

https://hackaday.com/2018/05/24/vcf-east-the-desktop-eniac/#more-308966

For modern analyses, it has become evident that two instruction set architectures, Intel/AMD x86_64 and ARM, underpin the vast majority of all computing due to market consolidation in the CPU space (Rosenthal, 3.2.4).  Within scientific computing in particular, this market consolidation is even more acute, with only 6.2% of the Top 500 supercomputers using architectures other than AMD/Intel's x86_64 as of June 2018 (see [here](https://www.top500.org/statistics/list/)). This means that, relative to the heterogeneity of hardware architectures in the 90s, the total space of hardware we would need to emulate in the future is quite small for a modern scientific analysis.  Furthermore, it is abundantly clear that the majority of Top 500 HPC clusters are running Linux, which indicates that there are very few degrees of freedom when it comes to operating system choice in modern scientific environments.  

In his blog, Brown cites how some researchers have proposed co-opting the software development concept of "continuous integration" as a potential solution for the reproducibility crisis, re-running analyses constantly as data is recorded.  While this concept is intriguing, I'd suggest that scientists adapt another concept from the software development world, [code coverage](https://en.wikipedia.org/wiki/Code_coverage), but with the "coverage" element not relating to how many functions in their code have tests, but rather how many components of their software/hardware stack will be likely be emulatable in the future.  While it's not certain that someone will write emulators that implement every single feature of an instruction set architecture in the future, this kind of "emulation coverage" might be informed by factors such as a) how niche the hardware is, b) how large the community of end-users is, and c) how well-documented the hardware's interfaces and underlying implementation are.

Beyond software-based emulation of older hardware, there's also been a growing trend of using FPGAs to implement older hardware directly.  There are numerous advantages to this approach, from better utilization of resources such as electricity and compute cycles (e.g., many cycles are idle when you're emulating a legacy machine on a powerful machine and nothing else is running in the background), to the ability to run hardware in cloud instances such as Amazon's [F1 instance types](https://aws.amazon.com/ec2/instance-types/f1/), and finally the ability to more closely moderate the speed of execution (deliberately slowing it down if necessary).  A good example of this in the education space is Stephan Edwards's [Apple2fpga](http://www.cs.columbia.edu/~sedwards/apple2fpga/), which re-implements an Apple II+ using an FPGA board.  This example also illustrates how the Apple II might be considered a platform with good emulation coverage, since as Edwards notes on his site:

> The Apple II has been documented in great detail. Starting with the first Apple II "Redbook" Reference Manual, Apple itself has published the schematics for the Apple II series. When Woz spoke at Columbia, he mentioned this was intentional: he wanted to share as much technical information as possible to educate the users. 

It should be noted that the trend towards FPGAs is in a way an example of how "everything old is new again".  At VCF East 2018, an exhibit entitled "[Microcomputers With an Identity Crisis](http://www.retrotechnology.com/vcfe18/vcfe_may18.html)" by Douglas Crawford, Chris Fala, and Todd George demonstrated how in the past, it used to be more common to have multiple hardware architectures running on a single machine as a way to mitigate against the large number of incompatible hardware architectures.  While the examples given at the exhibit were actually ASICs on add-on cards (such as the [Apple IIe compatibility card](https://en.wikipedia.org/wiki/Apple_IIe_Card) for the Mac Color Classic or the 486 included alongside the conventional PowerPC CPU in the [PowerMAC 6100](https://en.wikipedia.org/wiki/Power_Macintosh_6100).  These add-on cards resemble the FPGA cloud instances, in the sense that FPGA instances can run multiple architectures.  Even if x86_64 did become obsolete, it could be re-implemented in an FPGA and then run in the cloud on a machine simulating many architectures at once.

I find myself in agreement with Brown's assessment of Docker, in that treating scientific software and analysis piplines as black boxes is dangerous for reproducibility and reduces transparency, although admittedly it could be argued that [Dockerfiles](https://docs.docker.com/engine/reference/builder/) and [Docker Compose](https://docs.docker.com/compose/) fufill some of the requirements of inspectability that Brown advocates.  The primary issue I have with Dockerfiles for purposes of inspectability is that they aren't sufficiently portable, and can't be used to deploy to bare metal, virtual machines, or other containerization technologies.  Anecdotally, it seems that many researchers are in a rush to embrace Docker without considering that it could quite easily be supplanted by [Kata Containers](https://katacontainers.io/), [jails](https://en.wikipedia.org/wiki/FreeBSD_jail) or [zones](https://illumos.org/man/5/zones) should the tech industry embrace these alternative technologies.  This particular nitpick of mine is related to the "emulation coverage" I discussed earlier.

Instead of Dockerfiles, I think that the inspectability criterion should be fulfilled by configuration management tools such as [Ansible](https://www.ansible.com/), [Puppet](https://puppet.com/), [Chef](https://www.chef.io/chef/), and [Cfengine](https://cfengine.com/).  Configuration tools use a declarative syntax that allows you to specify exactly how your environment is set up (i.e., which packages are installed, which versions are used, which volumes are mounted, etc).  While this syntax varies between tools, it can be used to both apply a series of commands against a base installation of an operating system to bring an execution host to a desired state, and can also serve as documentation that future researchers can look at and remix.  Interestingly, Cfengine  was created with the explicit goal of setting up research environments for physicists in the early 90s, which has the interesting implication that a large number of computing environments from that time period might be very well-characterized.

Conda environment.yml files also fulfill inspectability in a similar way to configuration management tools.  In fact, this is what HHMI's [Binder](https://mybinder.readthedocs.io/en/latest/config_files.html#config-files) tool uses to construct Docker images.  At this point, it's important to note that the amount of effort needed to reproduce a pipeline doesn't always necessarily need to focus on OS-level dependencies.  If a pipeline is implemented solely in a higher-level language such as Python, a researcher may have considerably less work in terms of documenting his/her environment.  Indeed, Docker is basically to C what a Python virtualenv is to Python.

Well-documented workflows are important too.

Another point in this whole conversation about reproducibility, however, is how far we want to go with reproducing results.  Up until now, we've focused on talking about reproducing the CPU instructions / program that performs an analysis, but what about the data?  There's another fidelity at play here, instrumentation fidelity.  Are we going to re-create the exact same MRI machine or electron microscope to validate results in the future?  Many designs are proprietary in life sciences.  What if there are systematic flaws in data collection that affect an entire generation of analyses that we don't know about.

Data repositories aren't as well developed or standardized, though dat and others are trying to change this.

Near-literal reproducibility is desirable for a number of reasons:
* Auditability: With the increase in researcher fraud, it is becoming increasingly important that the workflows used by science be both transparent and repeatable.
* In many cases, it's not your money, it's the public's and they should be entitled to look at and learn from your analyses as an instructional exercise.  Also, you should be striving to rein in waste since you are technically accountable to the public.
* Analyses that don't work out aren't necessarily useless.  Saying that reproducibility has a half-life is dangerously reminiscent of saying that null results aren't useful.  It's entirely possible that a scientific paradigm could go down a garden path for a very long while, end up in a local minima, and then have to back up.  If there is a need to back up to the point before we went down a garden path, we would want to be able to go back to before the garden path and re-run analyses to determine our next directions.  Utility is relative with time, and what seems useless now could have great value in the future.  A lot of problem-solving in science is [hill climbing](https://en.wikipedia.org/wiki/Hill_climbing).
